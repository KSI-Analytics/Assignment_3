{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fe6c7f0",
   "metadata": {},
   "source": [
    "Implementing Huggingface streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30c6b137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting filelock (from datasets)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting numpy>=1.17 (from datasets)\n",
      "  Using cached numpy-2.0.2-cp39-cp39-win_amd64.whl.metadata (59 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Using cached pyarrow-19.0.1-cp39-cp39-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Using cached pandas-2.2.3-cp39-cp39-win_amd64.whl.metadata (19 kB)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp39-cp39-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Using cached aiohttp-3.11.18-cp39-cp39-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Using cached huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\sabri\\desktop\\a3\\assignment_3\\.venv\\lib\\site-packages (from datasets) (25.0)\n",
      "Collecting pyyaml>=5.1 (from datasets)\n",
      "  Using cached PyYAML-6.0.2-cp39-cp39-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets)\n",
      "  Using cached async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Using cached frozenlist-1.6.0-cp39-cp39-win_amd64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.4.3-cp39-cp39-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Using cached propcache-0.3.1-cp39-cp39-win_amd64.whl.metadata (11 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Using cached yarl-1.20.0-cp39-cp39-win_amd64.whl.metadata (74 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sabri\\desktop\\a3\\assignment_3\\.venv\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.32.2->datasets)\n",
      "  Using cached charset_normalizer-3.4.1-cp39-cp39-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.32.2->datasets)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.32.2->datasets)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.32.2->datasets)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\sabri\\desktop\\a3\\assignment_3\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sabri\\desktop\\a3\\assignment_3\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sabri\\desktop\\a3\\assignment_3\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Using cached datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Using cached aiohttp-3.11.18-cp39-cp39-win_amd64.whl (443 kB)\n",
      "Using cached huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Using cached multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "Using cached numpy-2.0.2-cp39-cp39-win_amd64.whl (15.9 MB)\n",
      "Using cached pyarrow-19.0.1-cp39-cp39-win_amd64.whl (25.5 MB)\n",
      "Using cached PyYAML-6.0.2-cp39-cp39-win_amd64.whl (162 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached pandas-2.2.3-cp39-cp39-win_amd64.whl (11.6 MB)\n",
      "Using cached xxhash-3.5.0-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp39-cp39-win_amd64.whl (102 kB)\n",
      "Using cached frozenlist-1.6.0-cp39-cp39-win_amd64.whl (120 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached multidict-6.4.3-cp39-cp39-win_amd64.whl (38 kB)\n",
      "Using cached propcache-0.3.1-cp39-cp39-win_amd64.whl (45 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Using cached yarl-1.20.0-cp39-cp39-win_amd64.whl (93 kB)\n",
      "Installing collected packages: pytz, xxhash, urllib3, tzdata, tqdm, pyyaml, pyarrow, propcache, numpy, multidict, idna, fsspec, frozenlist, filelock, dill, charset-normalizer, certifi, attrs, async-timeout, aiohappyeyeballs, yarl, requests, pandas, multiprocess, aiosignal, huggingface-hub, aiohttp, datasets\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 async-timeout-5.0.1 attrs-25.3.0 certifi-2025.1.31 charset-normalizer-3.4.1 datasets-3.5.0 dill-0.3.8 filelock-3.18.0 frozenlist-1.6.0 fsspec-2024.12.0 huggingface-hub-0.30.2 idna-3.10 multidict-6.4.3 multiprocess-0.70.16 numpy-2.0.2 pandas-2.2.3 propcache-0.3.1 pyarrow-19.0.1 pytz-2025.2 pyyaml-6.0.2 requests-2.32.3 tqdm-4.67.1 tzdata-2025.2 urllib3-2.4.0 xxhash-3.5.0 yarl-1.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b9b05cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sabri\\Desktop\\A3\\Assignment_3\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating full split: 100%|██████████| 112590/112590 [00:20<00:00, 5461.05 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits dict_keys(['full'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating full split: 701528 examples [00:16, 43293.37 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits dict_keys(['full'])\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "#check for the types of splits in the dataset\n",
    "dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_meta_All_Beauty\")\n",
    "print(f\"Dataset splits {dataset.keys()}\")\n",
    "\n",
    "dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_review_All_Beauty\")\n",
    "print(f\"Dataset splits {dataset.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75b082e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to access the chunk of data from the All_Beauty category\n",
      "Finished accessing the chunck of data from the All_Beauty category 1/34 completed\n",
      "Starting to access the chunk of data from the Amazon_Fashion category\n",
      "Finished accessing the chunck of data from the Amazon_Fashion category 2/34 completed\n",
      "Starting to access the chunk of data from the Appliances category\n",
      "Finished accessing the chunck of data from the Appliances category 3/34 completed\n",
      "Starting to access the chunk of data from the Arts_Crafts_and_Sewing category\n",
      "Finished accessing the chunck of data from the Arts_Crafts_and_Sewing category 4/34 completed\n",
      "Starting to access the chunk of data from the Automotive category\n",
      "Finished accessing the chunck of data from the Automotive category 5/34 completed\n",
      "Starting to access the chunk of data from the Baby_Products category\n",
      "Finished accessing the chunck of data from the Baby_Products category 6/34 completed\n",
      "Starting to access the chunk of data from the Beauty_and_Personal_Care category\n",
      "Finished accessing the chunck of data from the Beauty_and_Personal_Care category 7/34 completed\n",
      "Starting to access the chunk of data from the Books category\n",
      "Finished accessing the chunck of data from the Books category 8/34 completed\n",
      "Starting to access the chunk of data from the CDs_and_Vinyl category\n",
      "Finished accessing the chunck of data from the CDs_and_Vinyl category 9/34 completed\n",
      "Starting to access the chunk of data from the Cell_Phones_and_Accessories category\n",
      "Finished accessing the chunck of data from the Cell_Phones_and_Accessories category 10/34 completed\n",
      "Starting to access the chunk of data from the Clothing_Shoes_and_Jewelry category\n",
      "Finished accessing the chunck of data from the Clothing_Shoes_and_Jewelry category 11/34 completed\n",
      "Starting to access the chunk of data from the Digital_Music category\n",
      "Finished accessing the chunck of data from the Digital_Music category 12/34 completed\n",
      "Starting to access the chunk of data from the Electronics category\n",
      "Finished accessing the chunck of data from the Electronics category 13/34 completed\n",
      "Starting to access the chunk of data from the Gift_Cards category\n",
      "Finished accessing the chunck of data from the Gift_Cards category 14/34 completed\n",
      "Starting to access the chunk of data from the Grocery_and_Gourmet_Food category\n",
      "Finished accessing the chunck of data from the Grocery_and_Gourmet_Food category 15/34 completed\n",
      "Starting to access the chunk of data from the Handmade_Products category\n",
      "Finished accessing the chunck of data from the Handmade_Products category 16/34 completed\n",
      "Starting to access the chunk of data from the Health_and_Household category\n",
      "Finished accessing the chunck of data from the Health_and_Household category 17/34 completed\n",
      "Starting to access the chunk of data from the Health_and_Personal_Care category\n",
      "Finished accessing the chunck of data from the Health_and_Personal_Care category 18/34 completed\n",
      "Starting to access the chunk of data from the Home_and_Kitchen category\n",
      "Finished accessing the chunck of data from the Home_and_Kitchen category 19/34 completed\n",
      "Starting to access the chunk of data from the Industrial_and_Scientific category\n",
      "Finished accessing the chunck of data from the Industrial_and_Scientific category 20/34 completed\n",
      "Starting to access the chunk of data from the Kindle_Store category\n",
      "Finished accessing the chunck of data from the Kindle_Store category 21/34 completed\n",
      "Starting to access the chunk of data from the Magazine_Subscriptions category\n",
      "Finished accessing the chunck of data from the Magazine_Subscriptions category 22/34 completed\n",
      "Starting to access the chunk of data from the Movies_and_TV category\n",
      "Finished accessing the chunck of data from the Movies_and_TV category 23/34 completed\n",
      "Starting to access the chunk of data from the Musical_Instruments category\n",
      "Finished accessing the chunck of data from the Musical_Instruments category 24/34 completed\n",
      "Starting to access the chunk of data from the Office_Products category\n",
      "Finished accessing the chunck of data from the Office_Products category 25/34 completed\n",
      "Starting to access the chunk of data from the Patio_Lawn_and_Garden category\n",
      "Finished accessing the chunck of data from the Patio_Lawn_and_Garden category 26/34 completed\n",
      "Starting to access the chunk of data from the Pet_Supplies category\n",
      "Finished accessing the chunck of data from the Pet_Supplies category 27/34 completed\n",
      "Starting to access the chunk of data from the Software category\n",
      "Finished accessing the chunck of data from the Software category 28/34 completed\n",
      "Starting to access the chunk of data from the Sports_and_Outdoors category\n",
      "Finished accessing the chunck of data from the Sports_and_Outdoors category 29/34 completed\n",
      "Starting to access the chunk of data from the Subscription_Boxes category\n",
      "Finished accessing the chunck of data from the Subscription_Boxes category 30/34 completed\n",
      "Starting to access the chunk of data from the Tools_and_Home_Improvement category\n",
      "Finished accessing the chunck of data from the Tools_and_Home_Improvement category 31/34 completed\n",
      "Starting to access the chunk of data from the Toys_and_Games category\n",
      "Finished accessing the chunck of data from the Toys_and_Games category 32/34 completed\n",
      "Starting to access the chunk of data from the Video_Games category\n",
      "Finished accessing the chunck of data from the Video_Games category 33/34 completed\n",
      "Starting to access the chunk of data from the Unknown category\n",
      "Finished accessing the chunck of data from the Unknown category 34/34 completed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def huggingface_streaming_chunks_from_categories(dataset_name, config_name, split= \"full\", chunk_size= 10000):\n",
    "    \n",
    "    chunks = load_dataset(dataset_name, name=config_name, split=split, streaming=True)\n",
    "    \n",
    "    chunk= [sample for _, sample in zip(range(chunk_size), chunks)]\n",
    "\n",
    "    return Dataset.from_list(chunk)\n",
    "\n",
    "categories= [\n",
    "    \"All_Beauty\", \"Amazon_Fashion\", \"Appliances\", \"Arts_Crafts_and_Sewing\", \"Automotive\",\n",
    "    \"Baby_Products\", \"Beauty_and_Personal_Care\", \"Books\", \"CDs_and_Vinyl\",\n",
    "    \"Cell_Phones_and_Accessories\", \"Clothing_Shoes_and_Jewelry\", \"Digital_Music\", \"Electronics\",\n",
    "    \"Gift_Cards\", \"Grocery_and_Gourmet_Food\", \"Handmade_Products\", \"Health_and_Household\",\n",
    "    \"Health_and_Personal_Care\", \"Home_and_Kitchen\", \"Industrial_and_Scientific\", \"Kindle_Store\",\n",
    "    \"Magazine_Subscriptions\", \"Movies_and_TV\", \"Musical_Instruments\", \"Office_Products\",\n",
    "    \"Patio_Lawn_and_Garden\", \"Pet_Supplies\", \"Software\", \"Sports_and_Outdoors\",\n",
    "    \"Subscription_Boxes\", \"Tools_and_Home_Improvement\", \"Toys_and_Games\", \"Video_Games\", \"Unknown\"\n",
    "] \n",
    "  \n",
    "count = 1\n",
    "\n",
    "for category in categories:\n",
    "    print(f\"Starting to access the chunk of data from the {category} category\")\n",
    "\n",
    "    review_chunk= huggingface_streaming_chunks_from_categories(\"McAuley-Lab/Amazon-Reviews-2023\", f\"raw_review_{category}\")\n",
    "    review_chunk_data = pd.DataFrame(review_chunk)\n",
    "    review_chunk_data.to_csv(f\"{category}_review.csv\", index=False)\n",
    "\n",
    "    meta_chunk= huggingface_streaming_chunks_from_categories(\"McAuley-Lab/Amazon-Reviews-2023\", f\"raw_meta_{category}\")\n",
    "    meta_chunk_data = pd.DataFrame(meta_chunk)\n",
    "    meta_chunk_data.to_csv(f\"{category}_meta.csv\", index=False)\n",
    "\n",
    "    print(f\"Finished accessing the chunck of data from the {category} category {count}/34 completed\")\n",
    "    count = count + 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
